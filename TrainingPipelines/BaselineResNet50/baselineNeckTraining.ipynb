{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyOKtPqBSke8Vhccze65vKbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbKVJT4h31YK","executionInfo":{"status":"ok","timestamp":1694785740544,"user_tz":-120,"elapsed":264532,"user":{"displayName":"Tomas Slaven","userId":"11900321035202824891"}},"outputId":"292d71bc-cf09-40e6-ad57-428ff63e6bbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","=======================================================\n","Arguments:\n","image_dir: /content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/Images\n","train_csv: /content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/train.csv\n","validation_csv: /content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/val.csv\n","test_csv: /content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/test.csv\n","num_classes: 8\n","output_dir: /content/drive/My Drive/baselineNeckResults\n","output_file_prefix: res50\n","output_file_extension: .csv\n","=======================================================\n","\n","Batch size: 32\n","Learning Rate: 0.001\n","Resolution: 64\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:  0\n","\n","Training Label Accuracies:\n","Overall Accuracy: 10.0536\n","Train Loss: 0.4123\n","F1 Score: 8.7687\n","\n","Validation Overall Accuracy: 1.0753\n","Validation Loss: 0.5887\n","-------------------------------------------------\n","\n","Epoch:  1\n","\n","Training Label Accuracies:\n","Overall Accuracy: 8.5791\n","Train Loss: 0.3479\n","F1 Score: 5.6540\n","\n","Validation Overall Accuracy: 3.2258\n","Validation Loss: 0.9137\n","-------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Test Overall Accuracy: 1.0638\n","Test Loss: 0.9845\n","F1 Score: 16.7486\n","-------------------------------------------------\n","Test Overall Accuracy: 1.0638\n","Test Loss: 0.9845\n","F1 Score: 16.7486\n","-------------------------------------------------\n","Batch size: 32\n","Learning Rate: 0.001\n","Resolution: 128\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:  0\n","\n","Training Label Accuracies:\n","Overall Accuracy: 3.0831\n","Train Loss: 0.4313\n","F1 Score: 5.2344\n","\n","Validation Overall Accuracy: 0.0000\n","Validation Loss: 0.3562\n","-------------------------------------------------\n","\n","Epoch:  1\n","\n","Training Label Accuracies:\n","Overall Accuracy: 3.6193\n","Train Loss: 0.3526\n","F1 Score: 3.7968\n","\n","Validation Overall Accuracy: 1.0753\n","Validation Loss: 0.3194\n","-------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Test Overall Accuracy: 0.0000\n","Test Loss: 0.3507\n","F1 Score: 0.4717\n","-------------------------------------------------\n","Test Overall Accuracy: 0.0000\n","Test Loss: 0.4005\n","F1 Score: 0.9091\n","-------------------------------------------------\n"]}],"source":["#Training pipeline for resnet-50 classificaiton model by Tomas Slaven of University Of Cape Town\n","#ResNet architecture derived from https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from PIL import Image\n","import ast\n","import argparse\n","import os\n","\n","#Import Google Drive folder, assuming the use of google colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Declare highest_mean_accuracy and highest macro F1-Score as global variables\n","global highest_mean_accuracy\n","global highest_f1_score\n","highest_mean_accuracy = 0.0\n","highest_f1_score = 0.0\n","\n","\n","# Custom dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, image_dir, csv_file, transform=None):\n","        self.image_dir = image_dir\n","        self.data = pd.read_csv(csv_file, encoding=\"utf-8\")\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image_id = self.data.iloc[idx, 1]\n","        image_path = f\"{self.image_dir}/{image_id}.jpeg\"\n","        image = Image.open(image_path).convert(\"RGB\")\n","        label = torch.tensor(self.data.iloc[idx, 2:], dtype=torch.float32)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","#Function to load resnet model\n","def load_resnet50(num_classes):\n","    #model = models.resnet50(pretrained=True)\n","    model = models.resnet50(pretrained=False)\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, num_classes)\n","    return model\n","\n","# Function to load data\n","def load_data(image_dir, train_csv, validation_csv, test_csv, batch_size, resolution):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((resolution, resolution)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.3721, 0.3721, 0.3721], std=[0.1801, 0.1801, 0.1801]),\n","        ]\n","    )\n","\n","    train_dataset = CustomDataset(image_dir, train_csv, transform)\n","    validation_dataset = CustomDataset(image_dir, validation_csv, transform)\n","    test_dataset = CustomDataset(image_dir, test_csv, transform)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n","\n","    return train_loader, validation_loader, test_loader\n","\n","\n","# Function to train the model\n","def train_model(\n","    model, criterion, optimizer, train_loader, validation_loader, num_epochs,\n","    device, output_folder_path\n","):\n","    global highest_mean_accuracy\n","    global highest_f1_score\n","    highest_mean_accuracy = 0.0\n","    highest_f1_score = 0.0\n","    model.to(device)\n","    all_epoch_metrics = []\n","    val_epoch_metrics = []\n","    train_df = pd.DataFrame()\n","    val_df = pd.DataFrame()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0.0\n","        total_correct = 0\n","        label_correct = [0] * args.num_classes\n","        label_total = [0] * args.num_classes\n","        TP = [0] * args.num_classes\n","        FP = [0] * args.num_classes\n","        TN = [0] * args.num_classes\n","        FN = [0] * args.num_classes\n","        precision, specificity, sensi, FNR, F1, label_accuracy =  [], [], [], [], [], []\n","        print(\"Epoch: \", epoch)\n","        print(\"\")\n","\n","        for images, labels in train_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracies\n","            predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n","            total_correct += (predicted_labels == labels).all(dim=1).sum().item()\n","            correct_per_label = (predicted_labels == labels).sum(dim=0).tolist()\n","\n","            for i in range(args.num_classes):\n","                label_correct[i] += correct_per_label[i]\n","                label_total[i] += len(images)\n","\n","                #calc TP and FP\n","                true_positive = ((predicted_labels[:, i] == 1) & (labels[:, i] == 1)).sum().item()\n","                false_positive = ((predicted_labels[:, i] == 1) & (labels[:, i] == 0)).sum().item()\n","                true_negative = ((predicted_labels[:, i] == 0) & (labels[:, i] == 0)).sum().item()\n","                false_negative = ((predicted_labels[:, i] == 0) & (labels[:, i] == 1)).sum().item()\n","\n","                TP[i] += true_positive\n","                FP[i] += false_positive\n","                TN[i] += true_negative\n","                FN[i] += false_negative\n","\n","        #Calculate Label Specific Metrics\n","        for i in range(args.num_classes):\n","            label_accuracy.append(\n","                (label_correct[i] / label_total[i] if label_total[i] > 0 else 0)*100\n","            )\n","            precision.append((TP[i] / (TP[i] + FP[i]) if TP[i] + FP[i] > 0 else 0)*100)\n","            sensi.append((TP[i] / (TP[i] + FN[i]) if TP[i] + FN[i] > 0 else 0)*100)\n","            specificity.append((TN[i] / (TN[i] + FP[i]) if TN[i] + FP[i] > 0 else 0)*100)\n","            FNR.append((FN[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0)*100)\n","            F1.append( 2 * (precision[i] * sensi[i]) / (precision[i] + sensi[i]) if precision[i] + sensi[i] > 0 else 0)\n","\n","        # Calculate macro-averages and overall training accuracy\n","        train_loss /= len(train_loader.dataset)\n","        overall_accuracy = total_correct / len(train_loader.dataset)\n","        train_accuracy = overall_accuracy * 100\n","\n","        label_accuracy.insert(0, train_accuracy )\n","        precision.insert(0, (sum(precision) / args.num_classes))\n","        sensi.insert(0, (sum(sensi) / args.num_classes))\n","        specificity.insert(0, (sum(specificity) / args.num_classes))\n","        FNR.insert(0, (sum(FNR) / args.num_classes))\n","        F1_score = (sum(F1) / args.num_classes)\n","        F1.insert(0, (sum(F1) / args.num_classes))\n","\n","        # Create a dictionary for training metrics and save to DataFrame\n","        train_metrics_dict = {\n","            \"Epoch\": [epoch] * len(label_names),\n","            \"Type (TRAIN)\": label_names,\n","            \"Mean Accuracy (TRAIN)\": label_accuracy,\n","            \"Precision (TRAIN)\": precision,\n","            \"Sensitivity (TRAIN)\": sensi,\n","            \"Specificity (TRAIN)\": specificity,\n","            \"FNR (TRAIN)\": FNR,\n","            \"F1 Score (TRAIN)\": F1,\n","        }\n","        a = pd.DataFrame(train_metrics_dict)\n","        train_df = pd.concat([train_df, a], axis=0)\n","\n","        print(\"Training Label Accuracies:\")\n","        print(f\"Overall Accuracy: {train_accuracy:.4f}\")\n","        print(f\"Train Loss: {train_loss:.4f}\")\n","        print(f\"F1 Score: {F1_score:.4f}\")\n","        print(\"\")\n","\n","        # Validate the model and get validation metrics\n","        val_metrics_dict = validate_model(\n","            model, criterion, validation_loader, DEVICE, output_folder_path, epoch\n","        )\n","\n","        b = pd.DataFrame(val_metrics_dict)\n","        val_df = pd.concat([val_df, b], axis=0)\n","\n","\n","    # Concatenate all metric DataFrames vertically\n","    combined_df = pd.concat([train_df, val_df ], axis=1)\n","\n","    return combined_df\n","\n","\n","# Function to validate the model\n","def validate_model(model, criterion, validation_loader, device, output_folder_path, epoch):\n","    model.eval()\n","    global highest_mean_accuracy\n","    global highest_f1_score\n","\n","\n","    with torch.no_grad():\n","        validation_loss = 0.0\n","        total_correct = 0\n","        label_correct = [0] * args.num_classes\n","        label_total = [0] * args.num_classes\n","        TP = [0] * args.num_classes\n","        FP = [0] * args.num_classes\n","        TN = [0] * args.num_classes\n","        FN = [0] * args.num_classes\n","        precision, specificity, sensi, FNR, F1, label_accuracy =  [], [], [], [], [], []\n","\n","        for images, labels in validation_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            validation_loss += loss.item() * images.size(0)\n","\n","\n","            # Calculate accuracies\n","            predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n","            total_correct += (predicted_labels == labels).all(dim=1).sum().item()\n","            correct_per_label = (predicted_labels == labels).sum(dim=0).tolist()\n","\n","            for i in range(args.num_classes):\n","                label_correct[i] += correct_per_label[i]\n","                label_total[i] += len(images)\n","\n","                #calc TP and FP\n","                true_positive = ((predicted_labels[:, i] == 1) & (labels[:, i] == 1)).sum().item()\n","                false_positive = ((predicted_labels[:, i] == 1) & (labels[:, i] == 0)).sum().item()\n","                true_negative = ((predicted_labels[:, i] == 0) & (labels[:, i] == 0)).sum().item()\n","                false_negative = ((predicted_labels[:, i] == 0) & (labels[:, i] == 1)).sum().item()\n","\n","                TP[i] += true_positive\n","                FP[i] += false_positive\n","                TN[i] += true_negative\n","                FN[i] += false_negative\n","\n","        for i in range(args.num_classes):\n","\n","            label_accuracy.append(\n","                (label_correct[i] / label_total[i] if label_total[i] > 0 else 0)*100\n","            )\n","\n","            precision.append((TP[i] / (TP[i] + FP[i]) if TP[i] + FP[i] > 0 else 0)*100)\n","            sensi.append((TP[i] / (TP[i] + FN[i]) if TP[i] + FN[i] > 0 else 0)*100)\n","            specificity.append((TN[i] / (TN[i] + FP[i]) if TN[i] + FP[i] > 0 else 0)*100)\n","            FNR.append((FN[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0)*100)\n","            F1.append( 2 * (precision[i] * sensi[i]) / (precision[i] + sensi[i]) if precision[i] + sensi[i] > 0 else 0)\n","\n","\n","        # Calculate macro-averages and overall validation accuracy\n","        validation_loss /= len(validation_loader.dataset)\n","        overall_accuracy = total_correct / len(validation_loader.dataset)\n","        validation_accuracy = (\n","            overall_accuracy * 100\n","        )\n","\n","        label_accuracy.insert(0, validation_accuracy )\n","        precision.insert(0, (sum(precision) / args.num_classes))\n","        sensi.insert(0, (sum(sensi) / args.num_classes))\n","        specificity.insert(0, (sum(specificity) / args.num_classes))\n","        FNR.insert(0, (sum(FNR) / args.num_classes))\n","        F1.insert(0, (sum(F1) / args.num_classes))\n","\n","        #Create a dictionary for validation metrics and save to DataFrame\n","        val_metrics_dict = {\n","            \"Epoch\": [epoch] * len(label_names),\n","            \"Type (VAL)\": label_names,\n","            \"Mean Accuracy (VAL)\": label_accuracy,\n","            \"Precision (VAL)\": precision,\n","            \"Sensitivity (VAL)\": sensi,\n","            \"Specificity (VAL)\": specificity,\n","            \"FNR (VAL)\": FNR,\n","            \"F1 Score (VAL)\": F1,\n","        }\n","\n","        mean_accuracy = validation_accuracy\n","        f1_score_mean = F1[0]\n","\n","        # Update the highest mean accuracy and save the model if needed\n","        if ((mean_accuracy > highest_mean_accuracy) or (highest_mean_accuracy == 0.0)):\n","            highest_mean_accuracy = mean_accuracy\n","            model_save_path = os.path.join(output_folder_path, 'model_highest_mean_accuracy.pth')\n","            torch.save(model.state_dict(), model_save_path)\n","\n","        # Update the highest F1 score and save the model if needed\n","        if ((f1_score_mean > highest_f1_score) or (highest_f1_score == 0.0)):\n","            highest_f1_score = f1_score_mean\n","            model_save_path = os.path.join(output_folder_path, 'model_highest_f1_score.pth')\n","            torch.save(model.state_dict(), model_save_path)\n","\n","        # Print label accuracies and overall accuracy\n","        print(f\"Validation Overall Accuracy: {validation_accuracy:.4f}\")\n","        print(f\"Validation Loss: {validation_loss:.4f}\")\n","        print(\"-------------------------------------------------\")\n","        print(\"\")\n","\n","    return val_metrics_dict\n","\n","\n","# Function to test the model\n","def test_model(model, criterion, test_loader, device):\n","    model.eval()\n","    test_loss = 0.0\n","\n","    with torch.no_grad():\n","        total_correct = 0\n","        label_correct = [0] * args.num_classes\n","        label_total = [0] * args.num_classes\n","        TP = [0] * args.num_classes\n","        FP = [0] * args.num_classes\n","        TN = [0] * args.num_classes\n","        FN = [0] * args.num_classes\n","        precision, specificity, sensi, FNR, F1, label_accuracy =  [], [], [], [], [], []\n","\n","\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracies\n","            predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n","            total_correct += (predicted_labels == labels).all(dim=1).sum().item()\n","            correct_per_label = (predicted_labels == labels).sum(dim=0).tolist()\n","\n","            for i in range(args.num_classes):\n","                label_correct[i] += correct_per_label[i]\n","                label_total[i] += len(images)\n","\n","                #calc TP and FP\n","                true_positive = ((predicted_labels[:, i] == 1) & (labels[:, i] == 1)).sum().item()\n","                false_positive = ((predicted_labels[:, i] == 1) & (labels[:, i] == 0)).sum().item()\n","                true_negative = ((predicted_labels[:, i] == 0) & (labels[:, i] == 0)).sum().item()\n","                false_negative = ((predicted_labels[:, i] == 0) & (labels[:, i] == 1)).sum().item()\n","\n","                TP[i] += true_positive\n","                FP[i] += false_positive\n","                TN[i] += true_negative\n","                FN[i] += false_negative\n","\n","        for i in range(args.num_classes):\n","            label_accuracy.append(\n","                (label_correct[i] / label_total[i] if label_total[i] > 0 else 0)*100\n","            )\n","\n","            precision.append((TP[i] / (TP[i] + FP[i]) if TP[i] + FP[i] > 0 else 0)*100)\n","            sensi.append((TP[i] / (TP[i] + FN[i]) if TP[i] + FN[i] > 0 else 0)*100)\n","            specificity.append((TN[i] / (TN[i] + FP[i]) if TN[i] + FP[i] > 0 else 0)*100)\n","            FNR.append((FN[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0)*100)\n","            F1.append( 2 * (precision[i] * sensi[i]) / (precision[i] + sensi[i]) if precision[i] + sensi[i] > 0 else 0)\n","\n","        overall_accuracy = total_correct / len(test_loader.dataset)\n","        test_accuracy = (overall_accuracy * 100)  # Multiply by 100 to get percentage\n","        test_loss /= len(test_loader.dataset)\n","\n","        label_accuracy.insert(0, test_accuracy )\n","        precision.insert(0, (sum(precision) / args.num_classes))\n","        sensi.insert(0, (sum(sensi) / args.num_classes))\n","        specificity.insert(0, (sum(specificity) / args.num_classes))\n","        FNR.insert(0, (sum(FNR) / args.num_classes))\n","        F1_score = (sum(F1) / args.num_classes)\n","        F1.insert(0, (sum(F1) / args.num_classes))\n","        epoch = 1\n","\n","        #Create a dictionary for test metrics and save to DataFrame\n","        test_metrics_dict = {\n","            \"Epoch\": [epoch] * len(label_names),\n","            \"Type (TEST)\": label_names,\n","            \"Mean Accuracy (TEST)\": label_accuracy,\n","            \"Precision (TEST)\": precision,\n","            \"Sensitivity (TEST)\": sensi,\n","            \"Specificity (TEST)\": specificity,\n","            \"FNR (TEST)\": FNR,\n","            \"F1 Score (TEST)\": F1,\n","        }\n","\n","\n","        # Print label accuracies and overall accuracy\n","        print(f\"Test Overall Accuracy: {test_accuracy:.4f}\")\n","        print(f\"Test Loss: {test_loss:.4f}\")\n","        print(f\"F1 Score: {F1_score:.4f}\")\n","        print(\"-------------------------------------------------\")\n","\n","    return test_metrics_dict\n","\n","# Function to calculate class weights based on a CSV file\n","def calcWeights(csv_path):\n","    train_df = pd.read_csv(csv_path)\n","    columns = train_df.keys()\n","    columns = list(columns)\n","    columns.remove(\"Num\")\n","    columns.remove(\"ID\")\n","    pos_count = []\n","    neg_count = []\n","    pos_weights = []\n","    total = 746\n","    for column in columns:\n","        pos_count.append(train_df[column].sum())\n","        neg_count.append(total - (train_df[column].sum()))\n","        pos_weights.append((total - (train_df[column].sum())) / total)\n","\n","    return pos_weights\n","\n","\n","if __name__ == \"__main__\":\n","\n","    args = argparse.Namespace(\n","        image_dir=\"/content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/Images\",\n","        train_csv=\"/content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/train.csv\",\n","        validation_csv=\"/content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/val.csv\",\n","        test_csv=\"/content/drive/MyDrive/Medical data/Neck/Unlocalized/supervised data/test.csv\",\n","        num_classes=8,\n","        output_dir=\"/content/drive/My Drive/baselineNeckResults\",\n","        output_file_prefix=\"res50\",\n","        output_file_extension=\".csv\",\n","        )\n","\n","    print(\"=======================================================\")\n","    print(\"Arguments:\")\n","    for arg, value in vars(args).items():\n","        print(f\"{arg}: {value}\")\n","    print(\"=======================================================\")\n","    print(\"\")\n","\n","    #Initializations\n","    label_names = [\n","        \"Overall (Macro-Average)\",\n","        \"Alignment\",\n","        \"Soft_tissue_Swelling\",\n","        \"Listhesis\",\n","        \"Fracture\",\n","        \"Dislocation\",\n","        \"Spinous\",\n","        \"Other_Pathogens\",\n","        \"normal\",\n","    ]\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    pos_weights = torch.tensor(calcWeights(args.train_csv), device=DEVICE)\n","\n","    # Create the output directory if it doesn't exist\n","    os.makedirs(args.output_dir, exist_ok=True)\n","\n","    # Initial Set of Combinations\n","    hyperparameter_grid = [\n","        {\"bs\": 32, \"lr\": 0.001, \"res\": 64, \"epochs\": 2},\n","        {\"bs\": 32, \"lr\": 0.001, \"res\": 128, \"epochs\": 2},\n","        # {\"bs\": 32, \"lr\": 0.001, \"res\": 64, \"epochs\": 15},\n","        # {\"bs\": 32, \"lr\": 0.001, \"res\": 128, \"epochs\": 15},\n","        # {\"bs\": 64, \"lr\": 0.001, \"res\": 64, \"epochs\": 15},\n","        # {\"bs\": 64, \"lr\": 0.001, \"res\": 128, \"epochs\": 15},\n","        # {\"bs\": 32, \"lr\": 0.002, \"res\": 64, \"epochs\": 15},\n","        # {\"bs\": 32, \"lr\": 0.002, \"res\": 128, \"epochs\": 15},\n","        # {\"bs\": 64, \"lr\": 0.002, \"res\": 64, \"epochs\": 15},\n","        # {\"bs\": 64, \"lr\": 0.002, \"res\": 128, \"epochs\": 15},\n","        # {\"bs\": 32, \"lr\": 0.0015, \"res\": 96, \"epochs\": 15},\n","    ]\n","\n","    results_df = pd.DataFrame()\n","\n","\n","    for hyperparams in hyperparameter_grid:\n","\n","        #Initialize hyperparameters\n","        batch_size = hyperparams[\"bs\"]\n","        learning_rate = hyperparams[\"lr\"]\n","        resolution = hyperparams[\"res\"]\n","        epochs = hyperparams[\"epochs\"]\n","\n","        #initialize output directory\n","        output_df = pd.DataFrame()\n","        folder_name = f\"bs{batch_size}_lr{learning_rate:.5f}_res{resolution}_ep{epochs}\"\n","        output_folder_path = os.path.join(args.output_dir, folder_name)\n","        os.makedirs(output_folder_path, exist_ok=True)\n","        output_file_path = os.path.join(output_folder_path, \"output.csv\")\n","\n","        #Print Hyperparameters\n","        print(\"Batch size:\", batch_size)\n","        print(\"Learning Rate:\", learning_rate)\n","        print(\"Resolution:\", resolution)\n","        print(\"\")\n","\n","        # Load data with the current hyperparameter settings\n","        train_loader, validation_loader, test_loader = load_data(\n","            args.image_dir,\n","            args.train_csv,\n","            args.validation_csv,\n","            args.test_csv,\n","            batch_size,\n","            resolution,\n","        )\n","\n","        # Load the ResNet-50 model\n","        model = load_resnet50(args.num_classes)\n","\n","        # Define loss function and optimizer\n","        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n","        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","        # Train the model\n","        train_val_df = train_model(\n","            model,\n","            criterion,\n","            optimizer,\n","            train_loader,\n","            validation_loader,\n","            epochs,\n","            DEVICE,\n","            output_folder_path,\n","        )\n","        del model\n","\n","        # Test the best Mean Acc model\n","        best_mean_accuracy_model = load_resnet50(args.num_classes)\n","        best_mean_accuracy_model.load_state_dict(torch.load(os.path.join(output_folder_path, 'model_highest_mean_accuracy.pth')))\n","        best_mean_accuracy_model.to(DEVICE)\n","        mean_acc_test_metrics_dict = test_model(\n","            best_mean_accuracy_model, criterion, test_loader, DEVICE\n","        )\n","        del best_mean_accuracy_model\n","\n","        #Test best F1-Score Model\n","        best_f1_score_model = load_resnet50(args.num_classes)\n","        best_f1_score_model.load_state_dict(torch.load(os.path.join(output_folder_path, 'model_highest_f1_score.pth')))\n","        best_f1_score_model.to(DEVICE)\n","        macro_F1_test_metrics_dict = test_model(\n","            best_f1_score_model, criterion, test_loader, DEVICE\n","        )\n","        del best_f1_score_model\n","\n","        #accumulate results\n","        mean_acc_test_df = pd.DataFrame(mean_acc_test_metrics_dict)\n","        mean_F1_test_df = pd.DataFrame(macro_F1_test_metrics_dict)\n","        test_df = pd.concat([mean_acc_test_df, mean_F1_test_df ], axis=1)\n","        csv_df = pd.concat([train_val_df, test_df ], axis=1)\n","\n","        #save individual model results\n","        csv_df.to_csv(output_file_path, index=False)\n","\n","        #Find best Mean Accuracy and F1 Score\n","        overall_mean_acc = mean_acc_test_metrics_dict[\"Mean Accuracy (TEST)\"][0]\n","        mac_f1 =  mean_acc_test_metrics_dict[\"F1 Score (TEST)\"][0]\n","        overall_macro_F1 = macro_F1_test_metrics_dict[\"F1 Score (TEST)\"][0]\n","        mac_acc = macro_F1_test_metrics_dict[\"Mean Accuracy (TEST)\"][0]\n","\n","        if mac_acc > overall_mean_acc:\n","            overall_mean_acc = mac_acc\n","\n","        if mac_f1 > overall_macro_F1:\n","            overall_macro_F1 = mac_f1\n","\n","        #Save Best Results to dictionary\n","        results_dict = {\n","        \"hyperparameters\": hyperparams,\n","        \"test_metrics_mean_acc\": overall_mean_acc,\n","        \"test_metrics_macro_F1\": overall_macro_F1\n","        }\n","\n","        #Append dictionary to other results for same hyperparameter search\n","        a_df = pd.DataFrame(results_dict)\n","        results_df = pd.concat([results_df, a_df ], axis=0)\n","\n","    #initilize hyperparameter results file\n","    output_file_name = \"GridSearchResults\"\n","    output_file_path = os.path.join(args.output_dir, output_file_name + \".csv\")\n","\n","    # Create a DataFrame from the hyperparameter search results and save to a CSV file\n","    results_df.to_csv(output_file_path, index=False)\n"]}]}